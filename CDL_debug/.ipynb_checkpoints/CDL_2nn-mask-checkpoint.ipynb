{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import time\n",
    "import numpy as np\n",
    "import pickle\n",
    "from numpy.linalg import det\n",
    "\n",
    "import CMINE_lib as CMINE\n",
    "# from Guassian_variables import Data_guassian\n",
    "\n",
    "import pandas as pd\n",
    "from scipy.stats import multivariate_normal\n",
    "import itertools\n",
    "\n",
    "np.random.seed(37)\n",
    "from scipy import stats\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "import math\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_sum_exp(value, dim=None, keepdim=False):\n",
    "    \"\"\"Numerically stable implementation of the operation\n",
    "    value.exp().sum(dim, keepdim).log()\n",
    "    \"\"\"\n",
    "    # TODO: torch.max(value, dim=None) threw an error at time of writing\n",
    "    if dim is not None:\n",
    "        m, _ = torch.max(value, dim=dim, keepdim=True)\n",
    "        value0 = value - m\n",
    "        if keepdim is False:\n",
    "            m = m.squeeze(dim)\n",
    "        return m + torch.log(torch.sum(torch.exp(value0),\n",
    "                                       dim=dim, keepdim=keepdim))\n",
    "    else:\n",
    "        m = torch.max(value)\n",
    "        sum_exp = torch.sum(torch.exp(value - m))\n",
    "        if isinstance(sum_exp, Number):\n",
    "            return m + math.log(sum_exp)\n",
    "        else:\n",
    "            return m + torch.log(sum_exp)\n",
    "\n",
    "class L1OutUB(nn.Module):  # naive upper bound\n",
    "    def __init__(self, x_dim, y_dim, hidden_size):\n",
    "        super(L1OutUB, self).__init__()\n",
    "        y_dim = 1\n",
    "        self.p_mu = nn.Sequential(nn.Linear(x_dim, hidden_size//2),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.Linear(hidden_size//2, y_dim))\n",
    "\n",
    "        self.p_logvar = nn.Sequential(nn.Linear(x_dim, hidden_size//2),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.Linear(hidden_size//2, y_dim),\n",
    "                                       nn.Tanh())\n",
    "\n",
    "        self.p_mu_neg = nn.Sequential(nn.Linear(x_dim, hidden_size//2),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.Linear(hidden_size//2, y_dim))\n",
    "\n",
    "        self.p_logvar_neg = nn.Sequential(nn.Linear(x_dim, hidden_size//2),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.Linear(hidden_size//2, y_dim),\n",
    "                                       nn.Tanh())\n",
    "        self.linear_map = nn.Linear(1, x_dim)\n",
    "\n",
    "\n",
    "    def get_mu_logvar(self, x_samples):\n",
    "        mu = self.p_mu(x_samples)\n",
    "        logvar = self.p_logvar(x_samples)\n",
    "        return mu, logvar\n",
    "    \n",
    "    def get_mu_logvar_neg(self, x_samples, i = 0):\n",
    "          \n",
    "        mu = self.p_mu_neg(x_samples)\n",
    "        logvar = self.p_logvar_neg(x_samples)\n",
    "        return mu, logvar\n",
    "\n",
    "    def forward(self, x_samples, y_samples): # x_samples = s_t, a ; y_samples = s_{t+1}\n",
    "        batch_size = y_samples.shape[0]\n",
    "        #x_samples[:, 1].masked_fill_(x_samples[:, 1]!=0, float(0))\n",
    "        cmi_dims =[]\n",
    "        \n",
    "        x_samples = x_samples.unsqueeze(-1)\n",
    "        x_samples = self.linear_map(x_samples)\n",
    "        for k in range(y_samples.shape[1]):\n",
    "            max_values, max_indices = torch.max(x_samples, dim=1)\n",
    "            mu, logvar = self.get_mu_logvar(max_values)\n",
    "            \n",
    "\n",
    "            positive = (- (mu - y_samples[:,k].unsqueeze(-1))**2 /2./logvar.exp() - logvar/2.).sum(dim = -1) #[nsample]\n",
    "\n",
    "            negative = []\n",
    "            \n",
    "            for i in range(x_samples.shape[1]-1):\n",
    "                x_new = x_samples.masked_fill(x_samples.new_zeros(x_samples.size()).bool()[:, i, :], float('-inf'))\n",
    "                max_values, max_indices = torch.max(x_new, dim=1)\n",
    "\n",
    "                #x_temp = x_samples.index_fill_(1, torch.tensor([i]).cuda(), float('-inf'))\n",
    "                mu, logvar = self.get_mu_logvar_neg(max_values)\n",
    "                neg = (- (mu - y_samples[:,k].unsqueeze(-1))**2 /2./logvar.exp() - logvar/2.).sum(dim = -1) #[nsample]\n",
    "                if i == 0:\n",
    "                    negative = neg.unsqueeze(-1)\n",
    "                else:\n",
    "                    negative = torch.cat([negative, neg.unsqueeze(-1)], 1)\n",
    "                    \n",
    "            \n",
    "            cmi_dim = (positive.unsqueeze(-1)- negative ).mean()\n",
    "\n",
    "            cmi_dims.append(cmi_dim.abs().item())\n",
    "\n",
    "       \n",
    "        return cmi_dims\n",
    "    \n",
    "    def loglikeli(self, x_samples, y_samples):\n",
    "        x_samples = x_samples.clone()\n",
    "        y_samples = y_samples.clone()\n",
    "        \n",
    "        num = y_samples.shape[1]\n",
    "        for k in range(y_samples.shape[1]):\n",
    "        \n",
    "            mu, logvar = self.get_mu_logvar(x_samples)\n",
    "\n",
    "            lg = (-(mu - y_samples[:,k].unsqueeze(-1))**2 /logvar.exp()-logvar).sum(dim=1).mean(dim=0)\n",
    "            if  k == 0:\n",
    "                lgs = lg\n",
    "            else:\n",
    "                lgs += lg\n",
    "\n",
    "        del x_samples, y_samples\n",
    "        torch.cuda.empty_cache()\n",
    "        #print(\"lg\", lg)\n",
    "        return lgs/num\n",
    "    \n",
    "    def loglikeli_mask(self, x_samples, y_samples):\n",
    "        negative = []\n",
    "        x_samples = x_samples.clone()\n",
    "        y_samples = y_samples.clone()\n",
    "        num = y_samples.shape[1]\n",
    "        for k in range(y_samples.shape[1]):\n",
    "            for i in range(x_samples.shape[1]-1):\n",
    "                result = []\n",
    "\n",
    "                for j in range(x_samples.shape[1]): \n",
    "                    if j != i:\n",
    "                        result.append(j)\n",
    "                x_temp = torch.index_select(x_samples, dim=1, index=torch.tensor(result).cuda())\n",
    "                #x_temp = x_samples.index_fill_(1, torch.tensor([i]).cuda(), float('0'))\n",
    "                mu, logvar = self.get_mu_logvar_neg(x_temp)\n",
    "                neg =  (-(mu - y_samples[:,k].unsqueeze(-1))**2 /logvar.exp()-logvar).sum(dim=-1) #(- (mu - y_samples)**2 /2./logvar.exp() - logvar/2.).sum(dim = -1) #[nsample]\n",
    "                if i == 0:\n",
    "                    negative = neg.unsqueeze(-1)\n",
    "                else:\n",
    "                    negative = torch.cat([negative, neg.unsqueeze(-1)], 1)\n",
    "            if k == 0:\n",
    "                negatives = negative.sum(dim=1).mean(dim=0)\n",
    "            else:\n",
    "                negatives += negative.sum(dim=1).mean(dim=0)\n",
    "        del x_samples, y_samples\n",
    "        torch.cuda.empty_cache()\n",
    "        #print('mask', negative.sum(dim=1).mean(dim=0))\n",
    "        return negatives/num\n",
    "\n",
    "    def learning_loss(self, x_samples, y_samples):\n",
    "        return  - self.loglikeli_mask(x_samples, y_samples)  - self.loglikeli(x_samples, y_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "torch.backends.cudnn.enabled = True\n",
    "torch.backends.cudnn.benchmark = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dim = 5\n",
    "dataset = CMINE.create_dataset_DGP(GenModel=\"\", Params=\"\", Dim=5, N=64)\n",
    "s_t = torch.from_numpy(dataset[0]).float().cuda()\n",
    "s_next = torch.from_numpy(dataset[1]).float().cuda()\n",
    "a = torch.from_numpy(dataset[2]).float().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 10])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_next.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 11])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([s_t,a], dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (64) must match the size of tensor b (10) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_315/1309652619.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ms_next\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasked_fill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_next\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_zeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_next\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-inf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (64) must match the size of tensor b (10) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "s_next.masked_fill(s_next.new_zeros(s_next.size()).bool()[:, 0], float('-inf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = s_next.index_fill_(0, torch.tensor([1]).cuda(), float('-inf'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[       -inf, -3.4028e+38,        -inf,        -inf,        -inf,\n",
       "                -inf,        -inf,        -inf,        -inf,        -inf],\n",
       "        [       -inf, -3.4028e+38,        -inf,        -inf,        -inf,\n",
       "                -inf,        -inf,        -inf,        -inf,        -inf],\n",
       "        [       -inf, -3.4028e+38,        -inf,        -inf,        -inf,\n",
       "                -inf,        -inf,        -inf,        -inf,        -inf],\n",
       "        [       -inf, -3.4028e+38, -3.1061e-01, -1.1060e-02, -9.8567e-02,\n",
       "          2.4106e+00, -1.4879e-01, -3.5335e+00, -1.4129e+00,  8.1605e-01],\n",
       "        [       -inf, -3.4028e+38, -1.0710e-01, -1.0696e-01, -6.1144e-02,\n",
       "          2.5347e+00, -9.5286e-02, -3.5671e+00, -1.4442e+00,  1.0270e+00],\n",
       "        [       -inf, -3.4028e+38,  6.8417e-02,  1.0529e-02,  2.2792e-02,\n",
       "          2.2472e+00, -1.0071e-01, -3.4241e+00, -1.4355e+00,  9.2273e-01],\n",
       "        [       -inf, -3.4028e+38, -1.1767e-01, -2.5370e-02,  1.1199e-01,\n",
       "          2.5831e+00, -1.3244e-01, -3.5092e+00, -1.3493e+00,  8.4617e-01],\n",
       "        [       -inf, -3.4028e+38,  2.4402e-01, -5.6739e-02, -3.4736e-02,\n",
       "          2.2771e+00, -9.3235e-02, -3.7211e+00, -1.4422e+00,  8.6601e-01],\n",
       "        [       -inf, -3.4028e+38,  1.3427e-01,  9.0445e-02,  7.7107e-02,\n",
       "          2.6802e+00, -1.0873e-01, -3.4490e+00, -1.3368e+00,  9.1676e-01],\n",
       "        [       -inf, -3.4028e+38,  2.0802e-01,  5.1367e-02,  1.4128e-01,\n",
       "          2.6020e+00, -8.7389e-02, -3.4092e+00, -1.3728e+00,  8.0295e-01],\n",
       "        [       -inf, -3.4028e+38, -2.0208e-01,  4.0723e-02, -1.2653e-01,\n",
       "          2.6469e+00, -1.4190e-01, -3.5155e+00, -1.4379e+00,  8.5880e-01],\n",
       "        [       -inf, -3.4028e+38,  2.9634e-02, -4.6314e-02,  1.4853e-01,\n",
       "          2.7399e+00, -1.4760e-01, -3.4483e+00, -1.3564e+00,  8.9088e-01],\n",
       "        [       -inf, -3.4028e+38,  1.6801e-01,  3.4949e-03, -2.3831e-02,\n",
       "          2.5508e+00, -1.1834e-01, -3.4513e+00, -1.3296e+00,  8.6104e-01],\n",
       "        [       -inf, -3.4028e+38,  6.0215e-01,  2.4029e-01,  6.3486e-02,\n",
       "          2.9624e+00, -3.3113e-02, -3.3464e+00, -1.3130e+00,  7.8278e-01],\n",
       "        [       -inf, -3.4028e+38, -7.3897e-02,  6.8091e-02,  4.0938e-01,\n",
       "          2.6679e+00, -7.2987e-02, -3.4961e+00, -1.3122e+00,  7.9866e-01],\n",
       "        [       -inf, -3.4028e+38,  3.8454e-02, -1.1589e-01,  1.1854e-02,\n",
       "          2.3121e+00, -1.9740e-01, -3.4822e+00, -1.4007e+00,  8.5788e-01],\n",
       "        [       -inf, -3.4028e+38,  1.2471e-01, -8.8129e-02,  7.3781e-02,\n",
       "          2.5703e+00, -8.0151e-02, -3.5250e+00, -1.4702e+00,  9.0568e-01],\n",
       "        [       -inf, -3.4028e+38,  4.6313e-02, -2.1995e-02, -1.2332e-01,\n",
       "          2.5355e+00, -1.1723e-01, -3.4161e+00, -1.3931e+00,  8.1041e-01],\n",
       "        [       -inf, -3.4028e+38, -5.2571e-01, -2.2665e-01,  1.9485e-02,\n",
       "          2.3655e+00, -1.0591e-01, -3.4942e+00, -1.4598e+00,  9.8831e-01],\n",
       "        [       -inf, -3.4028e+38, -2.0135e-01, -2.6739e-02,  1.3131e-01,\n",
       "          2.3709e+00, -5.0246e-02, -3.4478e+00, -1.3485e+00,  9.5352e-01],\n",
       "        [       -inf, -3.4028e+38,  2.9157e-01,  1.2981e-01,  1.3098e-01,\n",
       "          2.4245e+00, -1.4426e-01, -3.4731e+00, -1.4087e+00,  8.7388e-01],\n",
       "        [       -inf, -3.4028e+38, -3.0130e-01,  1.3274e-02, -1.9411e-01,\n",
       "          1.9732e+00, -1.3964e-01, -3.5952e+00, -1.4938e+00,  9.4402e-01],\n",
       "        [       -inf, -3.4028e+38, -1.1094e-01,  3.2915e-02, -1.6461e-01,\n",
       "          2.2782e+00, -1.8560e-01, -3.6346e+00, -1.4690e+00,  9.6111e-01],\n",
       "        [       -inf, -3.4028e+38,  3.1107e-01,  8.7654e-02, -9.4433e-02,\n",
       "          2.2586e+00, -6.4918e-02, -3.5911e+00, -1.3706e+00,  8.8612e-01],\n",
       "        [       -inf, -3.4028e+38, -2.2180e-02, -4.7835e-02, -5.0116e-02,\n",
       "          2.4613e+00, -1.1791e-01, -3.5674e+00, -1.4262e+00,  8.2085e-01],\n",
       "        [       -inf, -3.4028e+38, -9.2496e-02,  7.1919e-02, -3.8948e-02,\n",
       "          2.3984e+00, -6.7372e-02, -3.6000e+00, -1.3901e+00,  8.4771e-01],\n",
       "        [       -inf, -3.4028e+38, -5.0293e-02,  5.0164e-02, -3.4970e-02,\n",
       "          2.5953e+00, -1.2582e-01, -3.3419e+00, -1.2824e+00,  8.2431e-01],\n",
       "        [       -inf, -3.4028e+38,  5.1901e-02, -8.7184e-02, -2.0408e-02,\n",
       "          2.7829e+00, -7.6903e-02, -3.4121e+00, -1.3090e+00,  9.7652e-01],\n",
       "        [       -inf, -3.4028e+38,  2.2394e-01,  1.1034e-01,  8.9658e-03,\n",
       "          2.5369e+00, -9.1959e-02, -3.3799e+00, -1.3255e+00,  8.1233e-01],\n",
       "        [       -inf, -3.4028e+38, -2.0196e-01, -3.4249e-02,  7.3101e-02,\n",
       "          2.5717e+00, -1.0890e-01, -3.3711e+00, -1.4231e+00,  9.1713e-01],\n",
       "        [       -inf, -3.4028e+38, -1.7492e-01, -1.0022e-01, -8.2637e-02,\n",
       "          2.4219e+00, -1.4685e-01, -3.5234e+00, -1.4733e+00,  9.4320e-01],\n",
       "        [       -inf, -3.4028e+38, -1.5219e-01, -7.8104e-02, -1.2605e-01,\n",
       "          2.3530e+00, -1.2001e-01, -3.4750e+00, -1.4632e+00,  8.9967e-01],\n",
       "        [       -inf, -3.4028e+38, -1.0750e-02, -3.3674e-02,  8.2677e-02,\n",
       "          2.2916e+00, -9.6045e-02, -3.4743e+00, -1.4438e+00,  8.8862e-01],\n",
       "        [       -inf, -3.4028e+38,  6.1070e-02, -8.0621e-02, -3.1915e-02,\n",
       "          2.4743e+00, -2.8780e-02, -3.5681e+00, -1.5437e+00,  9.1211e-01],\n",
       "        [       -inf, -3.4028e+38,  1.1244e-01, -1.0439e-01, -2.1941e-01,\n",
       "          2.5750e+00, -1.7303e-01, -3.5063e+00, -1.3922e+00,  8.6937e-01],\n",
       "        [       -inf, -3.4028e+38,  1.2527e-01, -8.0516e-02, -1.0069e-01,\n",
       "          2.5266e+00, -1.2210e-01, -3.4691e+00, -1.4002e+00,  8.8982e-01],\n",
       "        [       -inf, -3.4028e+38,  3.1225e-01,  1.9876e-01, -2.7043e-02,\n",
       "          2.8195e+00, -1.0547e-01, -3.3606e+00, -1.2229e+00,  7.6488e-01],\n",
       "        [       -inf, -3.4028e+38, -1.5429e-01, -7.4443e-02,  4.1391e-02,\n",
       "          2.6235e+00, -1.0862e-01, -3.5488e+00, -1.3881e+00,  9.3859e-01],\n",
       "        [       -inf, -3.4028e+38, -6.8576e-02,  3.7005e-03,  1.0754e-01,\n",
       "          2.7298e+00, -1.6310e-02, -3.4348e+00, -1.3773e+00,  8.4667e-01],\n",
       "        [       -inf, -3.4028e+38, -1.1247e-01, -1.5441e-01, -1.6687e-01,\n",
       "          2.2903e+00, -5.2522e-02, -3.4694e+00, -1.4579e+00,  9.6882e-01],\n",
       "        [       -inf, -3.4028e+38, -1.2951e-01,  1.3299e-01, -2.5996e-02,\n",
       "          2.3518e+00, -3.3602e-02, -3.6236e+00, -1.4571e+00,  9.2358e-01],\n",
       "        [       -inf, -3.4028e+38,  8.7131e-02,  1.4385e-01, -1.0585e-01,\n",
       "          2.4166e+00, -1.7583e-01, -3.5300e+00, -1.4937e+00,  7.9583e-01],\n",
       "        [       -inf, -3.4028e+38, -1.0070e-02,  9.9934e-02,  5.6937e-02,\n",
       "          2.4668e+00, -1.0862e-01, -3.4898e+00, -1.3836e+00,  8.4142e-01],\n",
       "        [       -inf, -3.4028e+38, -1.1730e-01, -8.7650e-02, -1.3502e-01,\n",
       "          2.6808e+00, -1.2272e-01, -3.5562e+00, -1.5188e+00,  8.6732e-01],\n",
       "        [       -inf, -3.4028e+38,  1.1267e-01,  2.0492e-02, -1.8673e-01,\n",
       "          2.4996e+00, -1.1591e-01, -3.4434e+00, -1.3751e+00,  8.6123e-01],\n",
       "        [       -inf, -3.4028e+38, -1.4920e-01,  3.0035e-02, -2.8327e-01,\n",
       "          2.4600e+00, -7.1858e-02, -3.4124e+00, -1.4571e+00,  9.5121e-01],\n",
       "        [       -inf, -3.4028e+38,  8.6428e-02, -4.6081e-02, -4.9192e-02,\n",
       "          2.4183e+00, -9.0299e-02, -3.5926e+00, -1.4651e+00,  9.6976e-01],\n",
       "        [       -inf, -3.4028e+38,  1.0025e-01,  2.3900e-01,  6.2709e-02,\n",
       "          2.6519e+00, -8.3867e-02, -3.3412e+00, -1.2338e+00,  8.4374e-01],\n",
       "        [       -inf, -3.4028e+38,  1.4079e-01,  7.3059e-02, -2.1169e-02,\n",
       "          2.6390e+00, -1.3874e-01, -3.5143e+00, -1.4005e+00,  9.1121e-01],\n",
       "        [       -inf, -3.4028e+38, -9.6649e-02, -1.8163e-01,  5.7464e-02,\n",
       "          2.5452e+00, -9.9256e-02, -3.5912e+00, -1.4923e+00,  9.4918e-01],\n",
       "        [       -inf, -3.4028e+38, -6.1753e-01, -2.4461e-01,  1.5786e-01,\n",
       "          2.5542e+00, -1.0733e-01, -3.6596e+00, -1.5904e+00,  9.6788e-01],\n",
       "        [       -inf, -3.4028e+38, -8.5813e-02,  8.7077e-02, -9.7855e-02,\n",
       "          2.4199e+00, -8.4519e-02, -3.5783e+00, -1.4525e+00,  9.5282e-01],\n",
       "        [       -inf, -3.4028e+38,  2.8145e-02, -1.5009e-01,  7.6647e-02,\n",
       "          2.5438e+00, -1.7181e-01, -3.4837e+00, -1.4236e+00,  7.2021e-01],\n",
       "        [       -inf, -3.4028e+38,  1.0817e-01, -6.6276e-02, -9.5125e-02,\n",
       "          2.4422e+00, -1.7510e-01, -3.5813e+00, -1.5107e+00,  9.1467e-01],\n",
       "        [       -inf, -3.4028e+38, -1.2545e-01,  1.0460e-01,  1.0001e-01,\n",
       "          2.6126e+00, -1.9788e-01, -3.4941e+00, -1.4174e+00,  9.7859e-01],\n",
       "        [       -inf, -3.4028e+38, -2.3450e-01,  5.3844e-02,  1.7414e-02,\n",
       "          2.7454e+00, -1.7362e-01, -3.3907e+00, -1.3524e+00,  8.3465e-01],\n",
       "        [       -inf, -3.4028e+38,  8.6285e-03, -2.1225e-01,  7.1884e-02,\n",
       "          2.6973e+00, -4.3489e-02, -3.4739e+00, -1.4181e+00,  9.0147e-01],\n",
       "        [       -inf, -3.4028e+38, -1.7589e-02,  5.2383e-02, -4.9721e-02,\n",
       "          2.4892e+00, -4.2090e-02, -3.4388e+00, -1.4103e+00,  9.1880e-01],\n",
       "        [       -inf, -3.4028e+38,  3.3014e-02,  1.6574e-03, -2.0733e-03,\n",
       "          2.5789e+00, -6.8357e-02, -3.5170e+00, -1.3732e+00,  8.4970e-01],\n",
       "        [       -inf, -3.4028e+38,  1.3681e-01, -1.8781e-02, -1.2302e-01,\n",
       "          2.3788e+00, -1.8017e-01, -3.4112e+00, -1.3086e+00,  7.7561e-01],\n",
       "        [       -inf, -3.4028e+38, -9.0234e-02, -4.1220e-02,  1.6311e-01,\n",
       "          2.6251e+00, -1.3110e-01, -3.3535e+00, -1.2314e+00,  8.1536e-01],\n",
       "        [       -inf, -3.4028e+38,  1.7417e-01,  9.8485e-03, -1.2873e-02,\n",
       "          2.2143e+00, -6.5638e-02, -3.5613e+00, -1.4529e+00,  8.5228e-01],\n",
       "        [       -inf, -3.4028e+38, -3.4649e-01, -2.5604e-03,  3.3290e-01,\n",
       "          2.5287e+00, -8.9017e-02, -3.5449e+00, -1.4173e+00,  1.0645e+00],\n",
       "        [       -inf, -3.4028e+38,  4.2845e-02, -6.7490e-02,  2.8108e-02,\n",
       "          2.4922e+00, -2.3712e-02, -3.5430e+00, -1.3549e+00,  9.9571e-01]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim = 1  # 要替换的维度\n",
    "neg_inf = torch.finfo(a.dtype).min  # 负无穷的值，与张量的数据类型匹配\n",
    "\n",
    "a[:, dim] = neg_inf  # 使用fill_方法替换指定维度为负无穷\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13157.32668220311\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "sample_dim = 2*Dim\n",
    "batch_size = 64\n",
    "hidden_size = 15\n",
    "learning_rate = 0.005\n",
    "training_steps = 40\n",
    "\n",
    "cubic = False \n",
    "\n",
    "# %%\n",
    "model = L1OutUB(sample_dim + 1, sample_dim, hidden_size).cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), learning_rate)\n",
    "\n",
    "# %%\n",
    "\n",
    "# %%\n",
    "mi_est_values = []\n",
    "\n",
    "# %%\n",
    "for step in range(training_steps):\n",
    "    #batch_x, batch_y = sample_correlated_gaussian(rho, dim=sample_dim, batch_size = batch_size, to_cuda = True, cubic = cubic)\n",
    "    dataset = CMINE.create_dataset_DGP(GenModel=\"\", Params=\"\", Dim=Dim, N=64)\n",
    "    s_t = torch.from_numpy(dataset[0]).float().cuda()\n",
    "    s_next = torch.from_numpy(dataset[1]).float().cuda()\n",
    "    a = torch.from_numpy(dataset[2]).float().cuda()\n",
    "    \n",
    "    batch_x = torch.cat([s_t,a], dim=1)\n",
    "    batch_y = s_next\n",
    "    model.eval()\n",
    "    cmi = model(batch_x, batch_y)\n",
    "    mi_est_values.append(cmi)\n",
    "    #print(cmi)\n",
    "    # %%\n",
    "    model.train() \n",
    "\n",
    "    model_loss = model.learning_loss(batch_x, batch_y)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    model_loss.backward(retain_graph=True)\n",
    "    optimizer.step()\n",
    "\n",
    "    del batch_x, batch_y\n",
    "    torch.cuda.empty_cache()\n",
    "#print(\"finish training for %s with true MI value = %f\"%('LOO', 6.0))\n",
    "print(np.array(mi_est_values).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 11])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_x = torch.cat([s_t,a], dim=1)\n",
    "batch_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comments\n",
    "The results of:\n",
    "\n",
    "'''\n",
    "np.array(mi_est_values).mean(axis=0)\n",
    "\n",
    "array([3.03903936e+00, 5.72949738e+00, 2.74534021e+00, 2.70783820e+00,\n",
    "       2.98499811e+00, 2.96300891e+01, 5.01361554e+02, 4.31678332e+05,\n",
    "       3.39265994e+03, 2.90363561e+02])\n",
    "\n",
    "'''\n",
    "\n",
    "looks good as: S_t is combined with [Xex_t, Se_t] where Xex_t is only involed by themselves and Se_t can have infulence on next states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.90645596e+00, 6.18735178e+00, 4.83287817e+00, 4.83604507e+00,\n",
       "       5.17863629e+00, 6.03508307e+01, 2.44899889e+02, 1.18550852e+05,\n",
       "       9.75231513e+03, 2.93890788e+03])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(mi_est_values).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffscm_gpu",
   "language": "python",
   "name": "diffscm_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
