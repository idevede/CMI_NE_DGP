{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import time\n",
    "import numpy as np\n",
    "import pickle\n",
    "from numpy.linalg import det\n",
    "\n",
    "import CMINE_lib as CMINE\n",
    "# from Guassian_variables import Data_guassian\n",
    "\n",
    "import pandas as pd\n",
    "from scipy.stats import multivariate_normal\n",
    "import itertools\n",
    "\n",
    "np.random.seed(37)\n",
    "from scipy import stats\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "import math\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_sum_exp(value, dim=None, keepdim=False):\n",
    "    \"\"\"Numerically stable implementation of the operation\n",
    "    value.exp().sum(dim, keepdim).log()\n",
    "    \"\"\"\n",
    "    # TODO: torch.max(value, dim=None) threw an error at time of writing\n",
    "    if dim is not None:\n",
    "        m, _ = torch.max(value, dim=dim, keepdim=True)\n",
    "        value0 = value - m\n",
    "        if keepdim is False:\n",
    "            m = m.squeeze(dim)\n",
    "        return m + torch.log(torch.sum(torch.exp(value0),\n",
    "                                       dim=dim, keepdim=keepdim))\n",
    "    else:\n",
    "        m = torch.max(value)\n",
    "        sum_exp = torch.sum(torch.exp(value - m))\n",
    "        if isinstance(sum_exp, Number):\n",
    "            return m + math.log(sum_exp)\n",
    "        else:\n",
    "            return m + torch.log(sum_exp)\n",
    "\n",
    "class L1OutUB(nn.Module):  # naive upper bound\n",
    "    def __init__(self, x_dim, y_dim, hidden_size):\n",
    "        super(L1OutUB, self).__init__()\n",
    "        y_dim = 1\n",
    "        self.p_mu = nn.Sequential(nn.Linear(x_dim, hidden_size//2),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.Linear(hidden_size//2, y_dim))\n",
    "\n",
    "        self.p_logvar = nn.Sequential(nn.Linear(x_dim, hidden_size//2),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.Linear(hidden_size//2, y_dim),\n",
    "                                       nn.Tanh())\n",
    "\n",
    "        self.p_mu_neg = nn.Sequential(nn.Linear(x_dim-1, hidden_size//2),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.Linear(hidden_size//2, y_dim))\n",
    "\n",
    "        self.p_logvar_neg = nn.Sequential(nn.Linear(x_dim-1, hidden_size//2),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.Linear(hidden_size//2, y_dim),\n",
    "                                       nn.Tanh())\n",
    "\n",
    "\n",
    "    def get_mu_logvar(self, x_samples):\n",
    "        mu = self.p_mu(x_samples)\n",
    "        logvar = self.p_logvar(x_samples)\n",
    "        return mu, logvar\n",
    "    \n",
    "    def get_mu_logvar_neg(self, x_samples):\n",
    "        mu = self.p_mu_neg(x_samples)\n",
    "        logvar = self.p_logvar_neg(x_samples)\n",
    "        return mu, logvar\n",
    "\n",
    "    def forward(self, x_samples, y_samples): # x_samples = s_t, a ; y_samples = s_{t+1}\n",
    "        batch_size = y_samples.shape[0]\n",
    "        #x_samples[:, 1].masked_fill_(x_samples[:, 1]!=0, float(0))\n",
    "        cmi_dims =[]\n",
    "        \n",
    "        \n",
    "        for k in range(y_samples.shape[1]):\n",
    "            mu, logvar = self.get_mu_logvar(x_samples)\n",
    "            \n",
    "\n",
    "            positive = (- (mu - y_samples[:,k].unsqueeze(-1))**2 /2./logvar.exp() - logvar/2.).sum(dim = -1) #[nsample]\n",
    "\n",
    "            negative = []\n",
    "            for i in range(x_samples.shape[1]-1):\n",
    "                result = []\n",
    "                for j in range(x_samples.shape[1]): \n",
    "                    if j != i:\n",
    "                        result.append(j)\n",
    "\n",
    "                x_temp = torch.index_select(x_samples, dim=1, index=torch.tensor(result).cuda())\n",
    "                #x_temp = x_samples.index_fill_(1, torch.tensor([i]).cuda(), float('0'))\n",
    "                mu, logvar = self.get_mu_logvar_neg(x_temp)\n",
    "                neg = (- (mu - y_samples[:,k].unsqueeze(-1))**2 /2./logvar.exp() - logvar/2.).sum(dim = -1) #[nsample]\n",
    "                if i == 0:\n",
    "                    negative = neg.unsqueeze(-1)\n",
    "                else:\n",
    "                    negative = torch.cat([negative, neg.unsqueeze(-1)], 1)\n",
    "                    \n",
    "            \n",
    "            cmi_dim = (positive.unsqueeze(-1)- negative ).abs().mean(axis=0).cpu().detach().numpy()\n",
    "\n",
    "            cmi_dims.append(cmi_dim)\n",
    "\n",
    "        # mu_1 = mu.unsqueeze(1)          # [nsample,1,dim]\n",
    "        # logvar_1 = logvar.unsqueeze(1)\n",
    "        # y_samples_1 = y_samples.unsqueeze(0)            # [1,nsample,dim]\n",
    "        # all_probs =  (- (y_samples_1 - mu_1)**2/2./logvar_1.exp()- logvar_1/2.).sum(dim = -1)  #[nsample, nsample]\n",
    "\n",
    "        # diag_mask =  torch.ones([batch_size]).diag().unsqueeze(-1).cuda() * (-20.)\n",
    "        # negative = log_sum_exp(all_probs + diag_mask,dim=0) - np.log(batch_size-1.) #[nsample]\n",
    "        #print(( positive.unsqueeze(-1)- negative ).mean())\n",
    "       \n",
    "        return cmi_dims\n",
    "    \n",
    "    def loglikeli(self, x_samples, y_samples):\n",
    "        x_samples = x_samples.clone()\n",
    "        y_samples = y_samples.clone()\n",
    "        \n",
    "        num = y_samples.shape[1]\n",
    "        for k in range(y_samples.shape[1]):\n",
    "        \n",
    "            mu, logvar = self.get_mu_logvar(x_samples)\n",
    "\n",
    "            lg = (-(mu - y_samples[:,k].unsqueeze(-1))**2 /logvar.exp()-logvar).sum(dim=1).mean(dim=0)\n",
    "            if  k == 0:\n",
    "                lgs = lg\n",
    "            else:\n",
    "                lgs += lg\n",
    "\n",
    "        del x_samples, y_samples\n",
    "        torch.cuda.empty_cache()\n",
    "        #print(\"lg\", lg)\n",
    "        return lgs/num\n",
    "    \n",
    "    def loglikeli_mask(self, x_samples, y_samples):\n",
    "        negative = []\n",
    "        x_samples = x_samples.clone()\n",
    "        y_samples = y_samples.clone()\n",
    "        num = y_samples.shape[1]\n",
    "        for k in range(y_samples.shape[1]):\n",
    "            for i in range(x_samples.shape[1]-1):\n",
    "                result = []\n",
    "\n",
    "                for j in range(x_samples.shape[1]): \n",
    "                    if j != i:\n",
    "                        result.append(j)\n",
    "                x_temp = torch.index_select(x_samples, dim=1, index=torch.tensor(result).cuda())\n",
    "                #x_temp = x_samples.index_fill_(1, torch.tensor([i]).cuda(), float('0'))\n",
    "                mu, logvar = self.get_mu_logvar_neg(x_temp)\n",
    "                neg =  (-(mu - y_samples[:,k].unsqueeze(-1))**2 /logvar.exp()-logvar).sum(dim=-1) #(- (mu - y_samples)**2 /2./logvar.exp() - logvar/2.).sum(dim = -1) #[nsample]\n",
    "                if i == 0:\n",
    "                    negative = neg.unsqueeze(-1)\n",
    "                else:\n",
    "                    negative = torch.cat([negative, neg.unsqueeze(-1)], 1)\n",
    "            if k == 0:\n",
    "                negatives = negative.sum(dim=1).mean(dim=0)\n",
    "            else:\n",
    "                negatives += negative.sum(dim=1).mean(dim=0)\n",
    "        del x_samples, y_samples\n",
    "        torch.cuda.empty_cache()\n",
    "        #print('mask', negative.sum(dim=1).mean(dim=0))\n",
    "        return negatives/num\n",
    "\n",
    "    def learning_loss(self, x_samples, y_samples):\n",
    "        return  - self.loglikeli_mask(x_samples, y_samples)  - self.loglikeli(x_samples, y_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "torch.backends.cudnn.enabled = True\n",
    "torch.backends.cudnn.benchmark = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dim = 5\n",
    "dataset = CMINE.create_dataset_DGP(GenModel=\"\", Params=\"\", Dim=5, N=64)\n",
    "s_t = torch.from_numpy(dataset[0]).float().cuda()\n",
    "s_next = torch.from_numpy(dataset[1]).float().cuda()\n",
    "a = torch.from_numpy(dataset[2]).float().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 10])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_next.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 11])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([s_t,a], dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2778.7847\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "sample_dim = 2*Dim\n",
    "batch_size = 64\n",
    "hidden_size = 15\n",
    "learning_rate = 0.005\n",
    "training_steps = 40\n",
    "\n",
    "cubic = False \n",
    "\n",
    "# %%\n",
    "model = L1OutUB(sample_dim + 1, sample_dim, hidden_size).cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), learning_rate)\n",
    "\n",
    "# %%\n",
    "\n",
    "# %%\n",
    "mi_est_values = []\n",
    "\n",
    "# %%\n",
    "for step in range(training_steps):\n",
    "    #batch_x, batch_y = sample_correlated_gaussian(rho, dim=sample_dim, batch_size = batch_size, to_cuda = True, cubic = cubic)\n",
    "    dataset = CMINE.create_dataset_DGP(GenModel=\"\", Params=\"\", Dim=Dim, N=64)\n",
    "    s_t = torch.from_numpy(dataset[0]).float().cuda()\n",
    "    s_next = torch.from_numpy(dataset[1]).float().cuda()\n",
    "    a = torch.from_numpy(dataset[2]).float().cuda()\n",
    "    \n",
    "    batch_x = torch.cat([s_t,a], dim=1)\n",
    "    batch_y = s_next\n",
    "    model.eval()\n",
    "    cmi = model(batch_x, batch_y)\n",
    "    mi_est_values.append(cmi)\n",
    "    #print(cmi)\n",
    "    # %%\n",
    "    model.train() \n",
    "\n",
    "    model_loss = model.learning_loss(batch_x, batch_y)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    model_loss.backward(retain_graph=True)\n",
    "    optimizer.step()\n",
    "\n",
    "    del batch_x, batch_y\n",
    "    torch.cuda.empty_cache()\n",
    "#print(\"finish training for %s with true MI value = %f\"%('LOO', 6.0))\n",
    "print(np.array(mi_est_values).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmi_to_plot = np.array(mi_est_values).mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 10, 10)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(mi_est_values).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x7f3f31a1a6d0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.imshow(cmi_to_plot)\n",
    "# first dimension: rows; second: columns\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([23.203314 , 23.202826 , 23.202246 , 23.200102 , 23.195562 ,\n",
       "       23.467264 , 23.712902 ,  4.7498884, 11.768938 , 12.568868 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmi_to_plot[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmi_to_plot[cmi_to_plot<100]=0\n",
    "cmi_to_plot[cmi_to_plot>=100]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmi_to_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffscm_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
